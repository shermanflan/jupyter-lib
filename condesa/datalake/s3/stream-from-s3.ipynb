{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516f5cbf",
   "metadata": {},
   "source": [
    "# Stream S3 Content\n",
    "\n",
    "References:\n",
    "- [Overview](https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select)\n",
    "- [User Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html)\n",
    "- [Client.select_object_content](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content)\n",
    "\n",
    "To Do:\n",
    "- Consider [Smart Open](https://github.com/RaRe-Technologies/smart_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375572c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import gzip as gz\n",
    "import logging\n",
    "from os import environ\n",
    "from os.path import basename, splitext\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ac8e0",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb61b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s',\n",
    "                    datefmt='%I:%M:%S %p', level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe007cde",
   "metadata": {},
   "source": [
    "# Inspect Bucket via Filter\n",
    "- The `Marker` parameter seems to be designed to implement watermarks\n",
    "- **TODO**: The S3 objects LastModified attribute is persisted so we may be able to use this for incremental loading rather than parsing the dates manually\n",
    "\n",
    "### References\n",
    "- https://stackoverflow.com/a/58066657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99851b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=environ['P3_AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key=environ['P3_AWS_SECRET_ACCESS_KEY'],\n",
    "    aws_session_token=environ['P3_AWS_SESSION_TOKEN']\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "# s3 = boto3.resource('s3')  # equivalent\n",
    "ale_bucket = s3.Bucket('my-lake')\n",
    "\n",
    "pracs = [\n",
    "    \"unload/vendor1/parquet/\",\n",
    "]\n",
    "for prac in pracs:\n",
    "    object_count, min_date, max_date = 0, datetime.max, datetime.min\n",
    "    for summary in ale_bucket.objects.filter(Prefix=prac):\n",
    "        object_count += 1\n",
    "        cur_date = summary.last_modified.replace(tzinfo=None)\n",
    "        min_date = min(min_date, cur_date)\n",
    "        max_date = max(max_date, cur_date)\n",
    "    logger.info(f\"{prac}: {object_count} from {min_date.date()} to {max_date.date()}\")\n",
    "\n",
    "# TODO: Inspect sorting\n",
    "# for summary in (\n",
    "#     ale_bucket\n",
    "#     .objects\n",
    "#     .filter(\n",
    "#         Prefix='unload/vendor1/parquet/',\n",
    "#         Marker='unload/vendor1/parquet/data_summary_20211019'\n",
    "#     )\n",
    "#     .limit(count=7)\n",
    "# ):\n",
    "#     logger.info(f\"{summary.key} at {summary.last_modified}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639645d9",
   "metadata": {},
   "source": [
    "# Inspect Bucket\n",
    "- The `StartAfter` parameter seems to be designed to implement watermarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_hj_client = boto3.client('s3')\n",
    "exports = s3_hj_client.list_objects_v2(\n",
    "            Bucket='my-bucket',\n",
    "            Prefix='unload/parquet/data_file_202112'\n",
    "#             StartAfter='unload/parquet/summary_20211001.gz'\n",
    "        )\n",
    "\n",
    "for part in exports.get('Contents', []):\n",
    "    logger.info(f\"{basename(part['Key'])}, {part['LastModified']}, {part['Size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113411f",
   "metadata": {},
   "source": [
    "# Stream S3\n",
    "Use `Body` attribute. Also, `iter_lines()` would be an option if the content was unzipped. This could take the unzipped lines, and then write to SQL. If the existing streaming implementation turns out flaky, this is plan B.\n",
    "\n",
    "- Alternatively, could write to a local temp file as batches of CSV/JSON and then upload to S3 or use psql COPY FROM (file or INPUT)\n",
    "- Alternatively, same as above but instead using EventStream as a source.\n",
    "\n",
    "References:\n",
    "- https://kokes.github.io/blog/2018/07/26/s3-objects-streaming-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add21cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_hj_client = boto3.client('s3')\n",
    "\n",
    "allergy_columns = ['client_id', 'patient_id', 'type', 'description', 'onset_date', \n",
    "                   'resolved_date', 'severity', 'reaction_code', 'reaction', 'product_code']\n",
    "\n",
    "allergy_obj = s3_hj_client.get_object(\n",
    "    Bucket='my-bucket',\n",
    "    Key='unload/parquet/data_file_202112'\n",
    ")\n",
    "\n",
    "lines = 0\n",
    "body = allergy_obj['Body']\n",
    "\n",
    "with closing(body):\n",
    "    with gz.open(body, 'rb') as f:\n",
    "        for row in f:  # allergy_obj['Body'].iter_lines():\n",
    "            line = row.decode('utf-8')\n",
    "            kv = {k:v for (k, v) in zip(allergy_columns, line.split('|'))}\n",
    "            logger.info(f\"Raw Row data: {line}\")\n",
    "            lines += 1\n",
    "\n",
    "logger.info(f\"Result is complete: {lines} rows processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec06cf4",
   "metadata": {},
   "source": [
    "# Stream S3 Object Content\n",
    "Using `select_object_content` to stream content from a gzip-compressed S3 object\n",
    "\n",
    "References\n",
    "- [EventStream](https://botocore.amazonaws.com/v1/documentation/api/latest/reference/eventstream.html)\n",
    "- [Partial record handling](https://github.com/aws/aws-sdk-net/issues/1296#issuecomment-494998477)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c47a70",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0da95b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_hj_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    content = s3_hj_client.select_object_content(\n",
    "        Bucket='my-bucket',\n",
    "        Key=\"unload/parquet/data_file_202112\",\n",
    "        RequestProgress = {\n",
    "            'Enabled': False\n",
    "        },\n",
    "        ExpressionType=\"SQL\",\n",
    "        Expression=\"\"\"\n",
    "            SELECT  s3.*\n",
    "            FROM    s3object AS s3\n",
    "            --WHERE   SUBSTRING(s3._14, 1, 4) = '2017'\n",
    "            LIMIT 21\n",
    "        \"\"\",\n",
    "        InputSerialization = {\n",
    "            \"CSV\": {\n",
    "                \"FileHeaderInfo\": \"NONE\",\n",
    "                \"FieldDelimiter\": \"|\",\n",
    "                \"QuoteCharacter\": \"|\",\n",
    "            }, \n",
    "            \"CompressionType\": \"GZIP\"\n",
    "        },\n",
    "        OutputSerialization = {\"CSV\": {\"QuoteFields\": \"ALWAYS\"}},\n",
    "    )\n",
    "    \n",
    "    event_stream = content['Payload']\n",
    "    end_event_received = False\n",
    "    total = batches = 0\n",
    "    lines, line = [], []\n",
    "\n",
    "    for event in event_stream:\n",
    "        if 'Records' in event:\n",
    "            records = event['Records']['Payload'].decode('utf-8')\n",
    "            for r in records:\n",
    "                if r == '\\n':\n",
    "                    lines.append(''.join(line))\n",
    "                    logger.info(''.join(line))\n",
    "                    line = []\n",
    "                else:\n",
    "                    line.append(r)\n",
    "            batches += 1\n",
    "        elif 'End' in event:\n",
    "            end_event_received = True\n",
    "\n",
    "        if lines:\n",
    "            total += len(lines)\n",
    "            lines = []\n",
    "finally:\n",
    "    if event_stream:\n",
    "        event_stream.close()\n",
    "\n",
    "if not end_event_received:\n",
    "    logger.info(\"End event not received, request incomplete.\")\n",
    "else:\n",
    "    logger.info(f\"Result is complete: {total} rows processed in {batches} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b060f7",
   "metadata": {},
   "source": [
    "## Remove NUL Characters\n",
    "Existence of extra `0x00` characters in strings is not allowed in PostgreSQL.\n",
    "\n",
    "References:\n",
    "- https://stackoverflow.com/a/1348551\n",
    "- https://stackoverflow.com/a/7760752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490130ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "tail -1 data/medication_20210619_l5.csv | od -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff28e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sed -e 's/\\x00//g' data/medication_20210619_l5.csv | tail -1 | od -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296db2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "sed -e 's/\\x00//g' -i data/medication_20210619.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f4bd7",
   "metadata": {},
   "source": [
    "## Write As COPY FROM Batch\n",
    "References\n",
    "- [Parameterization](https://stackoverflow.com/a/1471178)\n",
    "- [Parameterization/Psycopg2](https://www.psycopg.org/docs/usage.html#passing-parameters-to-sql-queries)\n",
    "- [`copy_expert`](https://stackoverflow.com/a/34523707)\n",
    "- [`copy_expert`/Psycopg2](https://www.psycopg.org/docs/usage.html#using-copy-to-and-copy-from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b36fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "maehc_db_url = 'postgresql://username@pgsql-jupyter-lib:5432/psycodb'\n",
    "db_engine = create_engine(maehc_db_url, echo=False)\n",
    "c = db_engine.raw_connection()\n",
    "etl_date = '20210619'\n",
    "s3_object = f\"medication_{etl_date}.csv\"\n",
    "\n",
    "try:\n",
    "    with open(f'data/{s3_object}', 'rb') as f:\n",
    "        cursor = c.cursor()\n",
    "        \n",
    "        cursor.execute(\"TRUNCATE TABLE staging.medication\")\n",
    "        \n",
    "        # \n",
    "        cursor.copy_expert(\"\"\"\n",
    "            COPY staging.medication (\n",
    "                client_id, patient_id, id_ndc, id_rxnorm, id_other,\n",
    "                type, name, dose, form, route, sig_code, sig, duration,\n",
    "                frequency, start_date, stop_date, comment, quantity,\n",
    "                refill, fill_status, sample_indicator, generic_indicator,\n",
    "                brand_name_code, prescribed_npi, status, encounter_id,\n",
    "                client_medication_id, row_hash\n",
    "            )\n",
    "            FROM STDIN\n",
    "            WITH (FORMAT csv)\n",
    "            --WITH (FORMAT csv, FORCE_NULL ( billable_ind ))\n",
    "        \"\"\", f)\n",
    "        \n",
    "        logger.info(f\"Bulk COPY FROM: {cursor.rowcount}\")\n",
    "\n",
    "#         cursor.execute(\"\"\"\n",
    "#             UPDATE staging.result\n",
    "#                 SET source = %(source)s\n",
    "#         \"\"\"\n",
    "#         , {\"source\": f\"{splitext(s3_object)[0]}.gz\",}\n",
    "#         )\n",
    "\n",
    "#         logger.info(f\"Bulk UPDATE: {cursor.rowcount}\")\n",
    "        \n",
    "        c.commit()\n",
    "except Exception as e:\n",
    "    c.rollback()\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
