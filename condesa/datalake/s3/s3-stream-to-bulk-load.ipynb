{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516f5cbf",
   "metadata": {},
   "source": [
    "# Stream S3 Content\n",
    "Approach for streaming S3 compressed objects and bulk loading to SQL\n",
    "\n",
    "#### References:\n",
    "- [Overview](https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select)\n",
    "- [User Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html)\n",
    "- [Client.select_object_content](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content)\n",
    "\n",
    "#### To Do:\n",
    "- Consider [Smart Open](https://github.com/RaRe-Technologies/smart_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375572c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import gzip as gz\n",
    "import logging\n",
    "from os import environ, path\n",
    "import tempfile\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ac8e0",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb61b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s',\n",
    "                    datefmt='%I:%M:%S %p', level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c6e39",
   "metadata": {},
   "source": [
    "## Db Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f644ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "maehc_db_url = 'postgresql://localhost@pgsql-jupyter-lib:5432/psycodb'\n",
    "db_engine = create_engine(maehc_db_url, echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524285de",
   "metadata": {},
   "source": [
    "## S3 References\n",
    "- [EventStream](https://botocore.amazonaws.com/v1/documentation/api/latest/reference/eventstream.html)\n",
    "- [Partial record handling](https://github.com/aws/aws-sdk-net/issues/1296#issuecomment-494998477)\n",
    "- [Example](https://kokes.github.io/blog/2018/07/26/s3-objects-streaming-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc4546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_stream(s3_client, bucket: str, pattern: str, source_sql: str):\n",
    "    content = s3_client.select_object_content(\n",
    "        Bucket=bucket,\n",
    "        Key=pattern,\n",
    "        RequestProgress={\"Enabled\": False},\n",
    "        ExpressionType=\"SQL\",\n",
    "        Expression=source_sql,\n",
    "        InputSerialization={\n",
    "            \"CSV\": {\n",
    "                \"FileHeaderInfo\": \"NONE\",\n",
    "                \"FieldDelimiter\": \"|\",\n",
    "                \"QuoteCharacter\": \"|\",\n",
    "            },\n",
    "            \"CompressionType\": \"GZIP\",\n",
    "        },\n",
    "        OutputSerialization={\n",
    "            \"CSV\": {\n",
    "                \"QuoteFields\": \"ALWAYS\"\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return content[\"Payload\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf96764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(s3_stream, target_path: str) -> None:\n",
    "    end_event_received = False\n",
    "\n",
    "    with open(target_path, 'wb') as f:\n",
    "\n",
    "        for event in s3_stream:\n",
    "            if 'Records' in event:\n",
    "                f.write(event['Records']['Payload'])\n",
    "            elif 'End' in event:\n",
    "                end_event_received = True\n",
    "\n",
    "    if not end_event_received:\n",
    "        raise Exception(\"End event not received, request incomplete.\")\n",
    "    \n",
    "    s3_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9bc58",
   "metadata": {},
   "source": [
    "## `COPY FROM` References\n",
    "- [`copy_expert`](https://stackoverflow.com/a/34523707)\n",
    "- [`copy_expert`/Psycopg2](https://www.psycopg.org/docs/usage.html#using-copy-to-and-copy-from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sql_bulk(db_engine, source_path: str, target_sql: str) -> int:\n",
    "    try:\n",
    "        c = db_engine.raw_connection()\n",
    "        cursor = c.cursor()\n",
    "        \n",
    "        with open(source_path, 'rb') as f:\n",
    "            \n",
    "            cursor.copy_expert(target_sql, f)\n",
    "            c.commit()\n",
    "            \n",
    "            return cursor.rowcount\n",
    "    except Exception as e:\n",
    "        c.rollback()\n",
    "        raise e\n",
    "    finally:\n",
    "        c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f0039",
   "metadata": {},
   "source": [
    "## Stream S3 Object Content\n",
    "#### Approach\n",
    "- Use `select_object_content` to stream content from a gzip-compressed S3 object\n",
    "- Save as a CSV file to local disk\n",
    "- Use COPY FROM to bulk import the content to SQL\n",
    "\n",
    "#### References\n",
    "- [Parameterization](https://stackoverflow.com/a/1471178)\n",
    "- [Parameterization/Psycopg2](https://www.psycopg.org/docs/usage.html#passing-parameters-to-sql-queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77daba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "exports = s3_client.list_objects_v2(\n",
    "    Bucket='my-bucket',\n",
    "    Prefix='org/unload/data_file_002'\n",
    ")\n",
    "\n",
    "with db_engine.connect() as co:\n",
    "    co.execute(\"TRUNCATE TABLE staging.result_bulk\")\n",
    "\n",
    "load_count = 0\n",
    "\n",
    "for part in exports.get('Contents', []):\n",
    "\n",
    "    logger.info(f\"Stream file: {part['Key']}\")\n",
    "\n",
    "    event_stream = get_s3_stream(s3_client, \n",
    "                                 bucket=\"my-bucket\", \n",
    "                                 pattern=part['Key'], \n",
    "                                 source_sql=\"\"\"\n",
    "                                    SELECT * FROM s3object\n",
    "                                    --LIMIT 1795962\n",
    "                                \"\"\")\n",
    "    part_name = path.basename(part['Key']).split('.')[0]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "\n",
    "#         part_path = path.join('data', f'{part_name}.csv')\n",
    "        part_path = path.join(tmpdir, f'{part_name}.csv')\n",
    "        \n",
    "        to_csv(event_stream, part_path)\n",
    "\n",
    "        logger.info(f\"Downloaded file: {part_path}\")\n",
    "        \n",
    "        load_count += to_sql_bulk(\n",
    "            db_engine, \n",
    "            source_path=part_path, \n",
    "            target_sql=\"\"\"\n",
    "                COPY staging.result_bulk (\n",
    "                    lab_resultid, client_id, patient_id, order_number, \"date\", \"time\", code, \n",
    "                    \"value\", units, ref_range, abnormal, comment, observation_coding_system, \n",
    "                    observation_description, status, client_lab_result_id, row_hash, order_date,\n",
    "                    collection_date\n",
    "                )\n",
    "                FROM STDIN\n",
    "                WITH (FORMAT csv)\n",
    "            \"\"\"\n",
    "        )\n",
    "            \n",
    "        logger.info(f\"Loaded (staging.result_bulk): {load_count}\")\n",
    "\n",
    "# with db_engine.connect() as co:\n",
    "#     co.execute(\"UPDATE staging.result_bulk SET source = %(source)s\", \n",
    "#                {\"source\": part['Key'],})\n",
    "logger.info(f\"Total Loaded (staging.result_bulk): {load_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
